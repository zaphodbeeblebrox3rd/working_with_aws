AWSTemplateFormatVersion: '2010-09-09'
Resources:
  # Resources from aws-transcribe-autotranscription-lambda.yaml
  S3FileUploadRule:
    Type: "AWS::Events::Rule"
    Properties: 
      Description: "EventBridge rule to trigger on file upload to S3"
      EventPattern: 
        source: 
          - "aws.s3"
        detail-type: 
          - "Object Created"
      State: "ENABLED"
      Targets: 
        - Arn: !GetAtt LaunchTranscribeJobFunction.Arn
          Id: "LaunchTranscribeJobFunction"

  LaunchTranscribeJobFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Handler: "index.handler"
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import re
          from datetime import datetime

          def handler(event, context):
              # Log the entire event for debugging
              print("Received event: " + json.dumps(event, indent=2))
              
              s3 = boto3.client('s3')
              transcribe = boto3.client('transcribe')
              
              bucket_name = event['detail']['bucket']['name']
              object_key = event['detail']['object']['key']

              allowed_extensions = {'amr', 'flac', 'm4a', 'mp3', 'mp4', 'ogg', 'webm', 'wav'}
              file_extension = object_key.split('.')[-1].lower()
              
              if file_extension in allowed_extensions:
                  timestamp = datetime.now().strftime("%Y%m%d%H%M")
                  job_name = f"autotranscription_{bucket_name}_{object_key.replace('/', '_')}_{timestamp}"

                  # Sanitize job_name to remove invalid characters
                  job_name = re.sub(r'[^0-9a-zA-Z._-]', '_', job_name)
                  job_uri = f"s3://{bucket_name}/{object_key}"
                  
                  transcribe.start_transcription_job(
                      TranscriptionJobName=job_name,
                      Media={'MediaFileUri': job_uri},
                      MediaFormat=file_extension,
                      OutputBucketName=bucket_name,
                      OutputKey="Transcription_Output/",
                      IdentifyMultipleLanguages=True,
                      Settings={
                          'ShowSpeakerLabels': True,
                          'MaxSpeakerLabels': 5
                      }
                  )
                  
                  print(f"Started transcription job for {object_key} in bucket {bucket_name}")
              else:
                  print(f"File '{object_key}' in bucket '{bucket_name}' does not have a valid extension")
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f"File '{object_key}' in bucket '{bucket_name}' does not have a valid extension")
                  }
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Lambda function completed successfully')
              }
      Runtime: "python3.8"

  LaunchTranscribeJobFunctionPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !Ref LaunchTranscribeJobFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt S3FileUploadRule.Arn

  # Resources from aws-transcribe-docx-conversion-lambda.yaml
  S3ObjectCreationRule:
    Type: "AWS::Events::Rule"
    Properties: 
      Description: "EventBridge rule to trigger on S3 object creation"
      EventPattern: 
        source: 
          - "aws.s3"
        detail-type: 
          - "Object Created"
      State: "ENABLED"
      Targets: 
        - Arn: !GetAtt ConvertJsonToDocxFunction.Arn
          Id: "ConvertJsonToDocxFunction"

  ConvertJsonToDocxFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Handler: "index.handler"
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: "python3.9"
      Layers:
        - !Ref DocxLayer
      Timeout: 900
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from docx import Document

          s3 = boto3.client('s3')

          def format_timestamp(seconds):
              hours = int(seconds // 3600)
              minutes = int((seconds % 3600) // 60)
              seconds = int(seconds % 60)
              return f"{hours:02}:{minutes:02}:{seconds:02}"

          def handler(event, context):
              print(event)
              # Extract bucket name and object key from the event
              bucket_name = event['detail']['bucket']['name']
              object_key = event['detail']['object']['key']

              # Verify the file is JSON before attempting download
              if not object_key.endswith('.json'):
                  print(f"Skipping non-JSON file: {object_key}")
                  return

              # Download the JSON file from S3
              local_json_path = '/tmp/asrOutput.json'
              s3.download_file(bucket_name, object_key, local_json_path)
              
              # Load the JSON data
              with open(local_json_path, 'r') as file:
                  data = json.load(file)
              
              # Check if 'speaker_labels' key exists
              if 'speaker_labels' not in data['results']:
                  print(f"No speaker labels found in {object_key}")
                  return

              # Create a new Document
              doc = Document()
              doc.add_heading('Transcription', 0)
              
              # Add the transcription text
              for item in data['results']['transcripts']:
                  doc.add_paragraph(item['transcript'])
              
              # Save the DOCX file to a temporary location
              local_docx_path = '/tmp/word_transcription.docx'
              doc.save(local_docx_path)
              
              # Upload the DOCX file to S3
              output_key = object_key.replace('.json', '.docx')
              s3.upload_file(local_docx_path, bucket_name, output_key)
              
              print(f"Document saved to s3://{bucket_name}/{output_key}")
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Transcription to DOCX conversion completed')
              }

  DocxLayer:
    Type: "AWS::Lambda::LayerVersion"
    Properties:
      LayerName: "docx-layer"
      Description: "Layer containing python-docx and its dependencies"
      Content:
        S3Bucket: !Sub "sscs-transcribe-lowrisk-config-${AWS::AccountId}"
        S3Key: "docx_layer.zip"  # Replace with the path to your ZIP file in S3
      CompatibleRuntimes:
        - "python3.9"

  ConvertJsonToDocxFunctionPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !Ref ConvertJsonToDocxFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt S3ObjectCreationRule.Arn

  # Resources from aws-translate-autotranslation-lambda.yaml
  S3DocumentUploadRule:
    Type: "AWS::Events::Rule"
    Properties: 
      Description: "EventBridge rule to trigger on file upload to S3"
      EventPattern: 
        source: 
          - "aws.s3"
        detail-type: 
          - "Object Created"
      State: "ENABLED"
      Targets: 
        - Arn: !GetAtt LaunchTranslateJobFunction.Arn
          Id: "LaunchTranslateJobFunction"

  LaunchTranslateJobFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Handler: "index.handler"
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 900
      Code:
        ZipFile: |
          import boto3
          import time
          import json
          import csv
          from datetime import datetime
          import os
          from botocore.exceptions import ClientError
          import re

          account_id = boto3.client('sts').get_caller_identity()['Account']

          def handler(event, context):
            global s3
            s3 = boto3.resource('s3')
            s3_client = boto3.client('s3')

            client = boto3.client('translate')

            # This is the bucket where the object is uploaded, taken from EventBridge
            bucket_name = event['detail']['bucket']['name']
            print(f"Bucket name: {bucket_name}")

            # This is the uploaded object, also from EventBridge
            object_key = event['detail']['object']['key']
            print(f"Object key: {object_key}")

            # Check if the first prefix in the object key is "Translation_Input"
            if object_key.startswith("Translation_Input/"):
              print(f"Translation input directory confirmed: {object_key}")
            else:
              print(f"Not a translation input directory: {object_key}")
              return {
                'statusCode': 400,
                'body': json.dumps('This prefix is not for autotranslation.  Skipping...')
              }
            
            # Make sure that the object is located directly under the Translation_Input prefix and not nested in a sub-prefix
            if object_key.count('/') > 1:
              print(f"Object key is not directly under the Translation_Input prefix: {object_key}")
              return {
                'statusCode': 400,
                'body': json.dumps('This object is not directly under the Translation_Input prefix.  Skipping...')
              }

            # check if the file is a compatible document file type
            if object_key.endswith(('.txt', '.docx', '.xlsx', '.pptx', '.xlf')):
              print(f"Compatible document file found: {object_key}")
            else:
              print(f"Not a compatible document file: {object_key}")
              return {
                'statusCode': 400,
                'body': json.dumps('This file type is not compatible with autotranslation')
              }
          
            # Translate only accepts a prefix.  Create a unique prefix to avoid retranslating other files.
            unique_prefix = str(time.time_ns())
            source_file_key_with_slash = object_key + unique_prefix + '/'
            source_file_folder = 's3://' + bucket_name + '/' + source_file_key_with_slash
            print(f"Source file folder: {source_file_folder}") # Debug source file folder assignment
            # Create the new folder in S3 for the file to move
            print(f"Creating new unique prefix in S3: {source_file_folder}")
            s3_client.put_object(Bucket=bucket_name, Key=source_file_key_with_slash)

            # Get the filename from the object key
            filename = object_key.split('/')[-1] 
            print(f"Filename: {filename}") # debug filename

            # Get the destination key
            destination_key = source_file_key_with_slash + filename
            print(f"Destination key: {destination_key}") # debug destination key

            # Copy the object to the new subprefix
            print(f"Copying object from s3://{bucket_name}/{object_key} to s3://{bucket_name}/{source_file_key_with_slash + filename}")
            s3_client.copy_object(CopySource={'Bucket': bucket_name, 'Key': object_key}, Bucket=bucket_name, Key=destination_key)

            translation_destination_folder = 's3://' + bucket_name + '/' + 'Translation_Output' + '/' + filename
 
            # Determine the content type based on the file extension
            if object_key.endswith('.docx'):
              content_type = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            elif object_key.endswith('.xlsx'):
              content_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            elif object_key.endswith('.pptx'):
              content_type = 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
            elif object_key.endswith('.xlf'):
              content_type = 'application/xliff+xml'
            elif object_key.endswith('.html'):
              content_type = 'text/html'
            else:
              content_type = 'text/plain'

            job_name=bucket_name + 'Translation_Input' + object_key + unique_prefix
            print(f"Job name: {job_name}")
            sanitized_job_name = re.sub(r'[^a-z0-9-]', '', job_name)
 
            response = client.start_text_translation_job(
              JobName=sanitized_job_name,
              InputDataConfig={
                'S3Uri': source_file_folder,
                'ContentType': content_type  # Use the determined content type
              },
              OutputDataConfig={
                'S3Uri': translation_destination_folder
              },
              DataAccessRoleArn=os.environ['DATA_ACCESS_ROLE_ARN'],
              SourceLanguageCode='auto',
              TargetLanguageCodes=[
                'en'
              ]
            )
 
            return {
              'statusCode': 200,
              'body': json.dumps('Your Translate request has been submitted successfully')
            } 
      Runtime: "python3.8"
      Environment:
         Variables:
           DATA_ACCESS_ROLE_ARN: !GetAtt LambdaExecutionRole.Arn

  LaunchTranslateJobFunctionPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !Ref LaunchTranslateJobFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt S3DocumentUploadRule.Arn

  # Resources related to user logon logging and bucket management
  UserLogonRule:
    Type: "AWS::Events::Rule"
    Properties: 
      Description: "EventBridge rule to trigger on user console logon"
      EventPattern: 
        detail-type: 
          - "AWS Console Sign In via CloudTrail"
        source: 
          - "aws.signin"
        account: 
          - !Sub "${AWS::AccountId}"
        detail: 
          userIdentity: 
            accountId: 
              - !Sub "${AWS::AccountId}"
          eventSource: 
            - "signin.amazonaws.com"
          eventName: 
            - "ConsoleLogin"
          responseElements: 
            ConsoleLogin: 
              - "Success"
          eventType: 
            - "AwsConsoleSignIn"
      State: "ENABLED"
      Targets: 
        - Arn: !GetAtt CreateBucketFunction.Arn
          Id: "CreateBucketFunction"

  CreateBucketFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Handler: "index.handler"
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 900
      Code:
        ZipFile: |
          import json
          import boto3
          from datetime import datetime
          from botocore.exceptions import ClientError
          import re
          import os
          import random
          import string

          account_id = boto3.client('sts').get_caller_identity()['Account']
          log_bucket_name = f"authentication-logs-{account_id}"
          string.digits = '0123456789'

          # initialize the IAM and Lambda clients
          iam_client = boto3.client('iam')
          lambda_client = boto3.client('lambda')

          def get_principal_id_from_role_arn(role_arn):
              role_name = role_arn.split(':')[-1].split('/')[-1]
              print(f"Role name: {role_name}")
              role = iam_client.get_role(RoleName=role_name)
              print(f"Role: {role}")
              print(f"RoleId: {role['Role']['RoleId']}")
              return role['Role']['RoleId']
          
          def get_role_arn_from_lambda_function(lambda_arn):
              print(f"Lambda ARN: {lambda_arn}")
              function_name = lambda_arn.split(':')[-1]
              print(f"Function name: {function_name}")
              response = lambda_client.get_function(FunctionName=function_name)
              print(f"Response: {response}")
              role_arn = response['Configuration']['Role']
              print(f"Role ARN: {role_arn}")
              return role_arn

          def handler(event, context):
              s3 = boto3.client('s3')
              s3_client = boto3.client('s3')
              detail = event['detail']
              print(detail)

              # Gather the current date   
              current_date = datetime.now().strftime("%Y-%m-%d")

              # Gather the current time
              current_time = datetime.now().strftime("%H:%M:%S")

              # Get the username of the user from EventBridge
              user_principal_id = detail['userIdentity']['principalId']
              print(f"Principal ID: {user_principal_id}")
              username = user_principal_id.split(':')[1] if ':' in user_principal_id else user_principal_id
              print(f"Username: {username}")

              # Get the ARN of the user from EventBridge
              arn = detail['userIdentity']['arn']
              print(f"User ARN: {arn}")
              iam_role = arn.split('/')[-2]
              print(f"IAM Role: {iam_role}")

              # Get the source IP
              source_ip = detail['sourceIPAddress']

              # Get the user agent
              user_agent = detail['userAgent']

              # Create the log file name
              log_file_name = f"{current_date}_{username}.log"

              # Assemble the log entry
              log_entry = f"{current_time} - {account_id} - {username} - {source_ip} - {user_agent} - {iam_role}"
              print(log_entry)

              # Make sure the log bucket exists and create it if it doesn't
              try:
                s3_client.head_bucket(Bucket=log_bucket_name)
              except ClientError as e:
                if e.response['Error']['Code'] == '404':
                  print(f"Log bucket does not exist: {log_bucket_name}")
                  s3_client.create_bucket(Bucket=log_bucket_name)
                  print(f"Log bucket created: {log_bucket_name}")
                  raise e
              
              # Check the S3 bucket for the log file
              try:
                  # Try to read the existing log file
                  existing_log = s3_client.get_object(Bucket=log_bucket_name, Key=log_file_name)['Body'].read().decode('utf-8')
                  # Append the log entry to a new line
                  updated_log = existing_log + '\n' + log_entry
              except ClientError as e:
                  if e.response['Error']['Code'] == 'NoSuchKey':
                    # If the object does not exist, create it with the new log entry
                    updated_log = log_entry
                  else:
                    # If there is another error, raise it
                    raise e

              # Write the updated log back to the S3 object
              s3_client.put_object(Bucket=log_bucket_name, Key=log_file_name, Body=updated_log)

              # Sanitize the bucket name
              unformatted_user_bucket_name = f"{username}-{account_id}"
              print("Unformatted bucket name:", unformatted_user_bucket_name)
              sanitized_user_bucket_name = unformatted_user_bucket_name.lower()
              user_bucket_name = re.sub(r'[^a-z0-9-]', '', sanitized_user_bucket_name)
              print("Sanitized bucket name:", user_bucket_name)
              
              # Create the S3 bucket
              try:
                  s3.create_bucket(Bucket=user_bucket_name)
                  print("Created S3 bucket:", user_bucket_name)
              except ClientError as e:
                  if e.response['Error']['Code'] == 'AccessDenied':
                    print(f"Access denied when creating bucket: {user_bucket_name}. Continuing execution with new bucket name.")
                    
                    # Create a new bucket name with a random numeric suffix # reinstate if necessary
                    # new_bucket_name = user_bucket_name + '-' + ''.join(random.choices(string.digits, k=4))
                  
                    new_bucket_name = user_bucket_name + '-' + 'a'
                    print(f"New bucket name: {new_bucket_name}")
                    s3.create_bucket(Bucket=new_bucket_name)
                    print("Created S3 bucket:", new_bucket_name)
                    user_bucket_name = new_bucket_name
                  else:
                      print(f"Error creating bucket: {e}")
                      raise e

              # Remove block public access setting to allow bucket policy to work
              s3.put_public_access_block(
                Bucket=user_bucket_name,
                PublicAccessBlockConfiguration={
                    'BlockPublicAcls': False
                }
              )
              
              # Set bucket for EventBridge notifications
              try:
                  s3.put_bucket_notification_configuration(
                      Bucket=user_bucket_name,
                      NotificationConfiguration={
                          'EventBridgeConfiguration': {}
                      }
                  )
                  print(f"Configured bucket notifications for EventBridge for bucket: {user_bucket_name}")
              except ClientError as e:
                  if e.response['Error']['Code'] == 'AccessDenied':
                    print(f"Access denied when configuring bucket notifications for bucket: {user_bucket_name}. Continuing execution.")
                  else:
                      print(f"Error configuring bucket notifications: {e}")
                      raise e

              # Create the prefixes
              prefixes = ['Audio_Files/', 'Transcription_Output/', 'Translation_Input/', 'Translation_Output/']
              for prefix in prefixes:
                  try:
                      s3.put_object(Bucket=user_bucket_name, Key=prefix)
                      print("Created prefix:", prefix)
                  except ClientError as e:
                      if e.response['Error']['Code'] == 'AccessDenied':
                          print(f"Access denied on creating bucket prefixes")
                      else:
                          print(f"Error creating prefix {prefix}: {e}")
                          raise e
              
              # Set the Owner tag
              try:
                  s3.put_bucket_tagging(
                      Bucket=user_bucket_name,
                      Tagging={
                          'TagSet': [
                              {
                                  'Key': 'Owner',
                                  'Value': user_bucket_name
                              }
                          ]
                      }
                  )
                  print("Set bucket tag 'Owner' with value:", user_bucket_name)
              except ClientError as e:
                  if e.response['Error']['Code'] == 'AccessDenied':
                      print(f"Access denied on setting Owner tag")
                  else:
                      print(f"Error setting bucket tag: {e}")
                      raise e

              # Set the bucket policy with explicit deny to all but the user
              
              # Get the ARNs of the lambda functions
              Launch_Transcribe_Job_Function_Name=os.environ['LAUNCH_TRANSCRIBE_JOB_FUNCTION_ARN']
              Launch_Translate_Job_Function_Name=os.environ['LAUNCH_TRANSLATE_JOB_FUNCTION_ARN']
              Convert_Json_To_Docx_Function_Name=os.environ['CONVERT_JSON_TO_DOCX_FUNCTION_ARN']
              Launch_Transcribe_Job_Function_ARN=get_role_arn_from_lambda_function(Launch_Transcribe_Job_Function_Name)
              Launch_Translate_Job_Function_ARN=get_role_arn_from_lambda_function(Launch_Translate_Job_Function_Name)
              Convert_Json_To_Docx_Function_ARN=get_role_arn_from_lambda_function(Convert_Json_To_Docx_Function_Name)
              
              # Get the principalId for each lambda function
              launch_transcribe_job_function_principal_id = get_principal_id_from_role_arn(Launch_Transcribe_Job_Function_ARN)
              launch_translate_job_function_principal_id = get_principal_id_from_role_arn(Launch_Translate_Job_Function_ARN)
              convert_json_to_docx_function_principal_id = get_principal_id_from_role_arn(Convert_Json_To_Docx_Function_ARN)

              bucket_policy = {
                  "Version": "2012-10-17",
                  "Statement": [
                      {
                          "Effect": "Deny", # Reinstate this after testing
                          "Principal": "*",
                          "Action": "s3:*",
                          "Resource": [
                              f"arn:aws:s3:::{user_bucket_name}",
                              f"arn:aws:s3:::{user_bucket_name}/*"
                          ],
                          "Condition": {
                              "StringNotLike": {
                                  "aws:userId": [
                                      f"*:{username}",
                                      "configLambdaExecution",
                                      f"{launch_transcribe_job_function_principal_id}*",
                                      f"{launch_translate_job_function_principal_id}*",
                                      f"{convert_json_to_docx_function_principal_id}*",
                                      "AIDAV5AJYCQDBJCU2CNSX",
                                      "AIDA46ZDFPMC6GSCLEPVI"
                                  ]
                              }
                          }
                      }
                  ]
              }
              print(bucket_policy)
              
              try:
                  s3.put_bucket_policy(
                      Bucket=user_bucket_name,
                      Policy=json.dumps(bucket_policy)
                  )
                  print("Set bucket policy with explicit deny")
              except ClientError as e:
                  print(f"Error setting bucket policy: {e}")
                  raise e

              return {
                  'statusCode': 200,
                  'body': json.dumps('Bucket notification configuration set successfully')
              }
      Runtime: "python3.8"
      Environment:
        Variables:
          LAUNCH_TRANSCRIBE_JOB_FUNCTION_ARN: !GetAtt LaunchTranscribeJobFunction.Arn
          LAUNCH_TRANSLATE_JOB_FUNCTION_ARN: !GetAtt LaunchTranslateJobFunction.Arn
          CONVERT_JSON_TO_DOCX_FUNCTION_ARN: !GetAtt ConvertJsonToDocxFunction.Arn
          # CREATE_BUCKET_FUNCTION_ARN: !GetAtt CreateBucketFunction.Arn

  # Common resources
  LambdaExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service: "lambda.amazonaws.com"
            Action: "sts:AssumeRole"
          - Effect: "Allow"
            Principal:
              Service: "translate.amazonaws.com"
            Action: "sts:AssumeRole"
      Policies:
        - PolicyName: "LambdaS3Policy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "transcribe:*"
                  - "polly:*"
                  - "translate:*"
                  - "comprehend:*"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "s3:ListMyBuckets"
                  - "s3:CreateBucket"
                  - "s3:GetObject"
                  - "s3:PutObject"
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                  - "s3:PutEncryptionConfiguration"
                  - "s3:DeleteObject"
                  - "s3:GetObjectVersion"
                  - "s3:PutBucketNotification"
                  - "s3:PutBucketNotificationConfiguration"
                  - "s3:PutBucketTagging"
                  - "s3:PutBucketPolicy"
                  - "s3:PutPublicAccessBlock"
                  - "kms:Encrypt"
                  - "kms:Decrypt"
                  - "kms:ReEncrypt*"
                  - "kms:GenerateDataKey*"
                  - "kms:DescribeKey"
                Resource:
                  - "arn:aws:s3:::*"
                  - "arn:aws:s3:::*/*"
              - Effect: "Allow"
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "iam:GetRole"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "iam:PassRole"
                  - "iam:GetRole"
                  - "lambda:GetFunction"
                Condition:
                  StringEquals:
                    iam:PassedToService: translate.amazonaws.com
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "lambda:GetFunction"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "lambda:GetFunction"
                Resource: 
                  - !Sub "arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:*"

  EventBridgeInvokeCreateBucketLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !GetAtt CreateBucketFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
  
  EventBridgeInvokeLaunchTranscribeJobLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !GetAtt LaunchTranscribeJobFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"

  EventBridgeInvokeLaunchTranslateJobLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !GetAtt LaunchTranslateJobFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"

  EventBridgeInvokeConvertJsonToDocxLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      FunctionName: !GetAtt ConvertJsonToDocxFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"

Outputs:
  LaunchTranscribeJobFunctionArn:
    Description: "ARN of the Transcribe Job Lambda Function"
    Value: !GetAtt LaunchTranscribeJobFunction.Arn
  
  ConvertJsonToDocxFunctionArn:
    Description: "ARN of the DOCX Conversion Lambda Function"
    Value: !GetAtt ConvertJsonToDocxFunction.Arn

  LaunchTranslateJobFunctionArn:
    Description: "ARN of the Translate Job Lambda Function"
    Value: !GetAtt LaunchTranslateJobFunction.Arn

  CreateBucketFunctionArn:
    Description: "ARN of the Create Bucket Lambda Function"
    Value: !GetAtt CreateBucketFunction.Arn
  


